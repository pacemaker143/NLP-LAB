# Named Entity Recognition (NER) System

## Project Description
This project implements a **Named Entity Recognition (NER)** system designed to identify and classify named entities in real-world text such as news articles or social media content. The system performs **token-level sequence labeling** using a supervised learning approach and follows the standard **BIO (Begin–Inside–Outside)** tagging scheme.

The model is implemented using **PyTorch** and evaluates its performance using widely accepted classification metrics including **Accuracy, Precision, Recall, and F1-score**.

---

## Approach
The NER task is formulated as a **sequence labeling problem**, where each token in a sentence is assigned an entity tag. The workflow consists of the following steps:

1. **Text Representation**  
   Input sentences are tokenized and converted into numerical representations using a vocabulary index.

2. **Model Architecture**  
   - An embedding layer converts token indices into dense vectors.  
   - A bidirectional Long Short-Term Memory (BiLSTM) network captures both past and future contextual information for each token.  
   - A fully connected layer maps contextual representations to entity tag probabilities.

3. **Training**  
   The model is trained using **cross-entropy loss**, optimized with the Adam optimizer. Training is performed at the token level across all sentences.

4. **Evaluation**  
   Model predictions are evaluated against ground truth labels using:
   - Accuracy  
   - Precision  
   - Recall  
   - F1-score  

These metrics provide a comprehensive assessment of entity extraction performance.

---

## Key Features
- Token-level Named Entity Recognition  
- BIO tagging scheme  
- BiLSTM-based sequence model  
- Standard evaluation metrics for NER  
- Suitable for academic and experimental use

---

## Use Cases
- Information extraction from news articles  
- Entity identification in social media text  
- Academic demonstrations of sequence labeling models  
- Baseline NER system for NLP research

---

## Notes
This implementation is intended as a **clear and extensible baseline**. It can be enhanced further by incorporating advanced architectures such as Transformers, Conditional Random Fields (CRF), or by training on large-scale annotated datasets.

